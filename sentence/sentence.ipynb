{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Programs\\Python\\Python37\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer, util\n",
    "import jieba, nltk, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentences = ['我想要去加入一个经常旅游的团队。',\n",
    "            '我们团队平时会偶尔去旅游的',\n",
    "            '我是一个不喜欢旅游的人']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = model.encode(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 384)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 我想要去加入一个经常旅游的团队。\n",
      "Embedding: (384,)\n",
      "Sentence: 我们团队平时会偶尔去旅游的\n",
      "Embedding: (384,)\n",
      "Sentence: 我是一个不喜欢旅游的人\n",
      "Embedding: (384,)\n"
     ]
    }
   ],
   "source": [
    "for sentence, embedding in zip(sentences, embeddings):\n",
    "    print(\"Sentence:\", sentence)\n",
    "    print(\"Embedding:\", embedding.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6433\n",
      "0.3285\n"
     ]
    }
   ],
   "source": [
    "sim = util.cos_sim(embeddings[0], embeddings[1])\n",
    "print(\"{0:.4f}\".format(sim.tolist()[0][0]))\n",
    "sim = util.cos_sim(embeddings[2], embeddings[1])\n",
    "print(\"{0:.4f}\".format(sim.tolist()[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hits = util.semantic_search(queries_embeddings, corpus_embeddings, top_k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "job_text = '【岗位职责】：1.一起头脑风暴，基于一个现有应用场景，讨论更多产品细节或技术功能，或基于我们已有的技术内容，开动脑筋，协助挖掘更多的发明点。（每天参加两小时头脑风暴或者培训会议）2.根据我们提供的技术框架，查更多资料，补充一些技术细节。3.审阅我们已经写好的专利内容，润色语言，并撰写发明创新idea。发明领域包括但不限于跟物联网、信息、软件、电学、机械、生活等相关的应用，包括人工智能、出行、文旅、教育、医疗、酒店、办公、社区、租赁、运动、餐饮等等。你可以挑选一个工作方向进行完成。如果你有学习能力，我们会对你进行专业培训，让你熟悉发明甚至拥有自己的发明专利。【任职要求】：1.喜欢创新，头脑活跃，喜欢交流，乐于表达自己。2.偏理工科专业，软件、物联网、智能、计算机方向优先，优秀者可放宽至本科生；文科生但是学习过跟发明或者专利代理相关的知识的也可以尝试。3.有物联网或算法或信息分析能力的同学佳，不需要会编程，会写相关领域技术或者思维活跃有创造力就可以。【福利待遇】：1.创新挖掘，根据是否挖掘到成果，每次两小时，60-260元，根据挖掘到的想法数量质量决定。2.每一篇你参与创意挖掘或者撰写的发明，你都有机会作为共同发明人进行发明专利的发表。3.实习三个月，可以提供实习证明。4.公司提供专业培训、创新辅助系统、创意框架和撰写模板。5.参与头脑风暴或者创新培训的同学可以线上或线下参与（推荐线下），每周至少参与两次，每次至少两小时。'\n",
    "hunter_text = '信息与计算科学专业，c,c++有一定的基础知识，能够使用python进行编程，在校学习过数据库原理，'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_seg_words(text : str):\n",
    "    seg_list = jieba.lcut(text)\n",
    "    interpunctuations = [',', '.', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%', \n",
    "                    '，', '。', '：', '；', '？', '（', '）', '【', '】', '！', '￥', ' ', '、', '-']\n",
    "    seg_list = [word for word in seg_list if word not in interpunctuations]\n",
    "    stops = set(nltk.corpus.stopwords.words(\"chinese\"))\n",
    "    seg_list = [word for word in seg_list if word not in stops]\n",
    "    return seg_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\vmice\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 0.650 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "job_sentences = get_seg_words(job_text)\n",
    "hunter_sentences = get_seg_words(hunter_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# job_text = job_text.replace('。', '. ')\n",
    "# # res = nltk.tokenize.word_tokenize(job_text)\n",
    "# print(job_text)\n",
    "\n",
    "# sentence = nltk.sent_tokenize(job_text)\n",
    "# print(sentence)\n",
    "# # job_text = ''.join(re.findall(r'[\\u4e00-\\u9fa5]', job_text))\n",
    "\n",
    "# seg_list = jieba.lcut(job_text)\n",
    "# print(seg_list)\n",
    "\n",
    "# interpunctuations = [',', '.', ':', ';', '?', '(', ')', '[', ']', '&', '!', '*', '@', '#', '$', '%', \n",
    "#                     '，', '。', '：', '；', '？', '（', '）', '【', '】', '！', '￥', ' ']\n",
    "# seg_list = [word for word in seg_list if word not in interpunctuations]\n",
    "# print(seg_list)\n",
    "\n",
    "# stops = set(nltk.corpus.stopwords.words(\"chinese\"))\n",
    "# seg_list = [word for word in seg_list if word not in stops]\n",
    "# print(seg_list)\n",
    "# # text = nltk.Text(seg_list)\n",
    "\n",
    "# type_tag = nltk.pos_tag(seg_list)\n",
    "# print(type_tag)\n",
    "\n",
    "# cut_word = []\n",
    "# for word in seg_list:\n",
    "#     cut_word.append(nltk.stem.PorterStemmer().stem(word))\n",
    "# print(cut_word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hunter_text = hunter_text.replace('，', '').replace(',', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seg_list = jieba.lcut(hunter_text, cut_all=False)\n",
    "# print(seg_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.6277\n"
     ]
    }
   ],
   "source": [
    "sentences = [job_sentences, hunter_sentences]\n",
    "embeddings = model.encode(sentences)\n",
    "sim = util.cos_sim(embeddings[0], embeddings[1])\n",
    "print(\"{0:.4f}\".format(sim.tolist()[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4581\n"
     ]
    }
   ],
   "source": [
    "sentences = [job_text, hunter_text]\n",
    "embeddings = model.encode(sentences)\n",
    "sim = util.cos_sim(embeddings[0], embeddings[1])\n",
    "print(\"{0:.4f}\".format(sim.tolist()[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "workspace",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
